"""
Unified Benchmarking Script for Z3 vs Gurobi Ski Planners
========================================================

This script runs both Z3 and Gurobi planners on the same query and compares:
- Solution quality
- Execution time
- Cost optimization
- Feasibility
- API costs (LLM calls)

Usage:
    python testing_z3_gurobi.py --query "Plan a 5-day ski trip to Zermatt for 4 people with budget 5000 euros"
    python testing_z3_gurobi.py --batch_test --max_queries 10
    python testing_z3_gurobi.py --query_file queries.txt
"""

import os
import sys
import time
import json
import argparse
import traceback
from datetime import datetime
from typing import Dict, List, Tuple, Optional
import pandas as pd

# Add project paths
sys.path.append(os.path.abspath(os.path.join(os.getcwd(), "..")))
sys.path.append(os.path.abspath(os.path.join(os.getcwd(), "tools/planner")))
os.chdir(os.path.dirname(os.path.abspath(__file__)))

# Import both planners
from test_skiplanner_z3_working import pipeline_ski as pipeline_ski_z3
from test_skiplanner_gurobi import pipeline_ski as pipeline_ski_gurobi

# Import LLM client for pure LLM comparison
from test_skiplanner_gurobi import LLMClient

def pipeline_ski_pure_llm(query: str, mode: str, model_name: str, index: int, model_version: str = None, verbose: bool = False) -> str:
    """Pure LLM pipeline without any constraint solver"""
    try:
        # Initialize LLM client
        llm_client = LLMClient(model_name=model_version or model_name)
        
        # Create a comprehensive prompt for ski trip planning
        planning_prompt = f"""You are an expert ski trip planner. Based on the following query, create a detailed ski trip plan with REALISTIC cost estimates.

Query: {query}

Please provide a comprehensive ski trip plan that includes:
1. Destination and resort selection
2. Accommodation recommendations with specific costs per night
3. Ski slope recommendations based on skill level
4. Equipment rental suggestions with daily costs
5. Transportation options with specific costs
6. Lift ticket prices per person per day
7. Food and dining cost estimates
8. Detailed cost breakdown and total estimated cost

IMPORTANT: Use REALISTIC European market prices. For example:
- Budget hotels: ‚Ç¨80-120 per night for 2 people
- Mid-range hotels: ‚Ç¨150-250 per night for 2 people
- Equipment rental: ‚Ç¨25-40 per person per day
- Lift tickets: ‚Ç¨45-65 per person per day
- Car rental: ‚Ç¨40-80 per day
- Meals: ‚Ç¨25-50 per person per day

Format your response with clear cost breakdowns:
Accommodation Cost: ‚Ç¨X.XX
Equipment Cost: ‚Ç¨X.XX
Transportation Cost: ‚Ç¨X.XX
Lift Tickets Cost: ‚Ç¨X.XX
Food Cost: ‚Ç¨X.XX
Total Cost: ‚Ç¨X.XX

Make sure all costs add up correctly and consider budget constraints, group size, and specific requirements mentioned in the query.

PLAN:"""

        # Get LLM response
        if verbose:
            print("ü§ñ Calling Pure LLM for ski trip planning...")
        
        response = llm_client._query_api(planning_prompt)
        
        if response:
            # Format the response as a proper plan
            plan = f"""PURE LLM SKI TRIP PLAN:
{response}

Generated by: Pure LLM ({model_version or model_name})
No constraint solver used - based purely on LLM reasoning."""
            
            return plan
        else:
            return None
            
    except Exception as e:
        if verbose:
            print(f"Error in Pure LLM pipeline: {e}")
        return None

class SkiBenchmarkResult:
    """Container for benchmark results from a single planner"""
    def __init__(self, planner_name: str):
        self.planner_name = planner_name
        self.success = False
        self.execution_time = 0.0
        self.plan_text = ""
        self.error_message = ""
        self.total_cost = 0.0
        self.cost_breakdown = {}
        self.feasible = False
        self.llm_calls = 0
        self.destination = ""
        self.days = 0
        self.people = 0
        self.budget = 0
        
    def to_dict(self) -> Dict:
        """Convert to dictionary for JSON serialization"""
        return {
            "planner_name": self.planner_name,
            "success": self.success,
            "execution_time": self.execution_time,
            "plan_text": self.plan_text,
            "error_message": self.error_message,
            "total_cost": self.total_cost,
            "cost_breakdown": self.cost_breakdown,
            "feasible": self.feasible,
            "llm_calls": self.llm_calls,
            "destination": self.destination,
            "days": self.days,
            "people": self.people,
            "budget": self.budget
        }

class ThreeWayBenchmarkComparison:
    """Container for comparing Pure LLM vs Z3 vs Gurobi results"""
    def __init__(self, query: str, pure_llm_result: SkiBenchmarkResult, z3_result: SkiBenchmarkResult, gurobi_result: SkiBenchmarkResult):
        self.query = query
        self.pure_llm_result = pure_llm_result
        self.z3_result = z3_result
        self.gurobi_result = gurobi_result
        self.timestamp = datetime.now().isoformat()
        
    def analyze_comparison(self) -> Dict:
        """Analyze and compare the three results"""
        analysis = {
            "query": self.query,
            "timestamp": self.timestamp,
            "success_summary": {
                "pure_llm_success": self.pure_llm_result.success,
                "z3_success": self.z3_result.success,
                "gurobi_success": self.gurobi_result.success,
                "all_successful": self.pure_llm_result.success and self.z3_result.success and self.gurobi_result.success,
                "success_count": sum([self.pure_llm_result.success, self.z3_result.success, self.gurobi_result.success])
            },
            "performance_comparison": {},
            "cost_comparison": {},
            "winner": None,
            "recommendation": ""
        }
        
        # Performance comparison (only for successful runs)
        successful_results = []
        if self.pure_llm_result.success:
            successful_results.append(("Pure LLM", self.pure_llm_result.execution_time, self.pure_llm_result.total_cost))
        if self.z3_result.success:
            successful_results.append(("Z3", self.z3_result.execution_time, self.z3_result.total_cost))
        if self.gurobi_result.success:
            successful_results.append(("Gurobi", self.gurobi_result.execution_time, self.gurobi_result.total_cost))
        
        if successful_results:
            # Sort by execution time
            fastest = min(successful_results, key=lambda x: x[1])
            slowest = max(successful_results, key=lambda x: x[1])
            
            # Sort by cost (only if cost > 0)
            cost_results = [(name, time, cost) for name, time, cost in successful_results if cost > 0]
            if cost_results:
                cheapest = min(cost_results, key=lambda x: x[2])
                most_expensive = max(cost_results, key=lambda x: x[2])
            else:
                cheapest = None
                most_expensive = None
            
            analysis["performance_comparison"] = {
                "pure_llm_time": self.pure_llm_result.execution_time if self.pure_llm_result.success else None,
                "z3_time": self.z3_result.execution_time if self.z3_result.success else None,
                "gurobi_time": self.gurobi_result.execution_time if self.gurobi_result.success else None,
                "fastest": fastest[0],
                "slowest": slowest[0],
                "fastest_time": fastest[1],
                "slowest_time": slowest[1]
            }
            
            analysis["cost_comparison"] = {
                "pure_llm_cost": self.pure_llm_result.total_cost if self.pure_llm_result.success else None,
                "z3_cost": self.z3_result.total_cost if self.z3_result.success else None,
                "gurobi_cost": self.gurobi_result.total_cost if self.gurobi_result.success else None,
                "cheapest": cheapest[0] if cheapest else None,
                "most_expensive": most_expensive[0] if most_expensive else None,
                "cheapest_cost": cheapest[2] if cheapest else None,
                "most_expensive_cost": most_expensive[2] if most_expensive else None
            }
            
            # Determine winner based on multiple criteria
            if analysis["success_summary"]["success_count"] == 3:
                # All successful - compare performance and cost
                speed_winner = fastest[0]
                cost_winner = cheapest[0] if cheapest else None
                
                if cost_winner and cheapest and most_expensive:
                    lowest_cost = cheapest[2]
                    cost_diff = most_expensive[2] - cheapest[2]
                    time_diff = slowest[1] - fastest[1]
                    
                    # Check if there are multiple methods with the same lowest cost
                    cost_tied_methods = [name for name, time, cost in successful_results if abs(cost - lowest_cost) < 0.01]
                    
                    if len(cost_tied_methods) > 1:  # Multiple methods have same optimal cost
                        # Among tied methods, pick the fastest
                        tied_with_times = [(name, time) for name, time, cost in successful_results if abs(cost - lowest_cost) < 0.01]
                        fastest_tied = min(tied_with_times, key=lambda x: x[1])
                        analysis["winner"] = fastest_tied[0]
                        
                        if len(cost_tied_methods) == 2:
                            other_tied = [name for name in cost_tied_methods if name != fastest_tied[0]][0]
                            other_time = next(time for name, time, cost in successful_results if name == other_tied)
                            time_advantage = other_time - fastest_tied[1]
                            analysis["recommendation"] = f"{fastest_tied[0]} recommended - same optimal cost (‚Ç¨{lowest_cost:.2f}) but {time_advantage:.2f}s faster than {other_tied}"
                        else:
                            analysis["recommendation"] = f"{fastest_tied[0]} recommended - optimal cost (‚Ç¨{lowest_cost:.2f}) with fastest execution ({fastest_tied[1]:.2f}s)"
                    elif cost_diff > 500:  # Significant cost difference
                        analysis["winner"] = cost_winner
                        analysis["recommendation"] = f"{cost_winner} recommended for significant cost savings (‚Ç¨{cost_diff:.2f})"
                    elif speed_winner == cost_winner:
                        analysis["winner"] = speed_winner
                        analysis["recommendation"] = f"{speed_winner} is both fastest and cheapest"
                    elif time_diff > 10:  # Significant time difference
                        analysis["winner"] = speed_winner
                        analysis["recommendation"] = f"{speed_winner} recommended for significantly faster execution ({time_diff:.2f}s faster)"
                    else:
                        analysis["winner"] = cost_winner
                        analysis["recommendation"] = f"{cost_winner} recommended for lowest cost (‚Ç¨{cheapest[2]:.2f})"
                else:
                    analysis["winner"] = speed_winner
                    analysis["recommendation"] = f"{speed_winner} recommended for best performance"
            
            elif analysis["success_summary"]["success_count"] == 2:
                # Two successful - pick the better one
                if not self.pure_llm_result.success:
                    better = "Z3" if self.z3_result.total_cost < self.gurobi_result.total_cost else "Gurobi"
                    analysis["winner"] = better
                    analysis["recommendation"] = f"{better} recommended (Pure LLM failed)"
                elif not self.z3_result.success:
                    better = "Pure LLM" if self.pure_llm_result.total_cost < self.gurobi_result.total_cost else "Gurobi"
                    analysis["winner"] = better
                    analysis["recommendation"] = f"{better} recommended (Z3 failed)"
                else:  # Gurobi failed
                    better = "Pure LLM" if self.pure_llm_result.total_cost < self.z3_result.total_cost else "Z3"
                    analysis["winner"] = better
                    analysis["recommendation"] = f"{better} recommended (Gurobi failed)"
            
            elif analysis["success_summary"]["success_count"] == 1:
                # Only one successful
                if self.pure_llm_result.success:
                    analysis["winner"] = "Pure LLM"
                    analysis["recommendation"] = "Pure LLM is the only working solution"
                elif self.z3_result.success:
                    analysis["winner"] = "Z3"
                    analysis["recommendation"] = "Z3 is the only working solution"
                else:
                    analysis["winner"] = "Gurobi"
                    analysis["recommendation"] = "Gurobi is the only working solution"
            
            else:
                # All failed
                analysis["winner"] = "None"
                analysis["recommendation"] = "All methods failed - check query constraints"
        
        return analysis
    
    def to_dict(self) -> Dict:
        """Convert to dictionary for JSON serialization"""
        return {
            "query": self.query,
            "timestamp": self.timestamp,
            "pure_llm_result": self.pure_llm_result.to_dict(),
            "z3_result": self.z3_result.to_dict(),
            "gurobi_result": self.gurobi_result.to_dict(),
            "analysis": self.analyze_comparison()
        }

def run_single_planner(planner_func, planner_name: str, query: str, model_name: str, verbose: bool = False) -> SkiBenchmarkResult:
    """Run a single planner and capture results"""
    result = SkiBenchmarkResult(planner_name)
    
    try:
        start_time = time.time()
        
        # Handle different function signatures
        if planner_name == "Z3":
            # Z3 planner uses 'model' parameter
            plan_text = planner_func(
                query=query,
                mode="benchmark",
                model=model_name,
                index=1,
                model_version=model_name,
                verbose=verbose
            )
        elif planner_name == "Pure LLM":
            # Pure LLM planner uses 'model_name' parameter
            plan_text = planner_func(
                query=query,
                mode="benchmark",
                model_name=model_name,
                index=1,
                model_version=model_name,
                verbose=verbose
            )
        else:
            # Gurobi planner uses 'model_name' parameter
            plan_text = planner_func(
                query=query,
                mode="benchmark",
                model_name=model_name,
                index=1,
                model_version=model_name,
                verbose=verbose
            )
        
        end_time = time.time()
        result.execution_time = end_time - start_time
        
        if plan_text:
            result.success = True
            result.plan_text = plan_text
            result.feasible = True
            
            # Extract cost information from plan text
            result.total_cost = extract_cost_from_plan(plan_text)
            result.cost_breakdown = extract_cost_breakdown_from_plan(plan_text)
            
            # Extract other parameters
            result.destination = extract_destination_from_plan(plan_text)
            result.days = extract_days_from_plan(plan_text)
            result.people = extract_people_from_plan(plan_text)
            result.budget = extract_budget_from_plan(plan_text)
            
        else:
            result.success = False
            result.error_message = "No plan generated"
            result.feasible = False
            
    except Exception as e:
        result.success = False
        result.error_message = str(e)
        result.execution_time = time.time() - start_time if 'start_time' in locals() else 0
        result.feasible = False
        
        if verbose:
            print(f"Error in {planner_name}: {e}")
            traceback.print_exc()
    
    return result

def extract_cost_breakdown_from_plan(plan_text: str) -> Dict[str, float]:
    """Extract detailed cost breakdown from plan text"""
    import re
    
    breakdown = {}
    
    # Look for various cost categories
    cost_categories = {
        'accommodation': [r'Accommodation[^:]*:\s*‚Ç¨?(\d+(?:,\d{3})*(?:\.\d{2})?)', 
                         r'Hotel[^:]*:\s*‚Ç¨?(\d+(?:,\d{3})*(?:\.\d{2})?)',
                         r'Lodging[^:]*:\s*‚Ç¨?(\d+(?:,\d{3})*(?:\.\d{2})?)'],
        'equipment': [r'Equipment[^:]*:\s*‚Ç¨?(\d+(?:,\d{3})*(?:\.\d{2})?)', 
                     r'Ski Rental[^:]*:\s*‚Ç¨?(\d+(?:,\d{3})*(?:\.\d{2})?)',
                     r'Gear[^:]*:\s*‚Ç¨?(\d+(?:,\d{3})*(?:\.\d{2})?)'],
        'transportation': [r'Transportation[^:]*:\s*‚Ç¨?(\d+(?:,\d{3})*(?:\.\d{2})?)', 
                          r'Car[^:]*:\s*‚Ç¨?(\d+(?:,\d{3})*(?:\.\d{2})?)',
                          r'Travel[^:]*:\s*‚Ç¨?(\d+(?:,\d{3})*(?:\.\d{2})?)',
                          r'Car Rental[^:]*:\s*‚Ç¨?(\d+(?:,\d{3})*(?:\.\d{2})?)'],
        'lift_tickets': [r'Lift[^:]*:\s*‚Ç¨?(\d+(?:,\d{3})*(?:\.\d{2})?)', 
                        r'Ski Pass[^:]*:\s*‚Ç¨?(\d+(?:,\d{3})*(?:\.\d{2})?)',
                        r'Tickets[^:]*:\s*‚Ç¨?(\d+(?:,\d{3})*(?:\.\d{2})?)'],
        'food': [r'Food[^:]*:\s*‚Ç¨?(\d+(?:,\d{3})*(?:\.\d{2})?)', 
                r'Meals[^:]*:\s*‚Ç¨?(\d+(?:,\d{3})*(?:\.\d{2})?)',
                r'Dining[^:]*:\s*‚Ç¨?(\d+(?:,\d{3})*(?:\.\d{2})?)'],
    }
    
    for category, patterns in cost_categories.items():
        for pattern in patterns:
            matches = re.findall(pattern, plan_text, re.IGNORECASE | re.MULTILINE)
            if matches:
                try:
                    cost = float(matches[-1].replace(',', ''))
                    breakdown[category] = cost
                    break
                except (ValueError, IndexError):
                    continue
    
    return breakdown

def extract_cost_from_plan(plan_text: str) -> float:
    """Extract total cost from plan text with better parsing"""
    import re
    
    # First, try to calculate total from cost breakdown (more reliable for Gurobi)
    breakdown = extract_cost_breakdown_from_plan(plan_text)
    if breakdown:
        total_from_breakdown = sum(breakdown.values())
        if total_from_breakdown > 0:
            return total_from_breakdown
    
    # Look for patterns like "Total Cost: ‚Ç¨1234.56" or "Cost: ‚Ç¨1234"
    cost_patterns = [
        r'Total Cost:\s*‚Ç¨?(\d+(?:,\d{3})*(?:\.\d{2})?)',
        r'Total Budget:\s*‚Ç¨?(\d+(?:,\d{3})*(?:\.\d{2})?)',
        r'Cost:\s*‚Ç¨?(\d+(?:,\d{3})*(?:\.\d{2})?)',
        r'Total:\s*‚Ç¨?(\d+(?:,\d{3})*(?:\.\d{2})?)',
        r'Budget Used:\s*‚Ç¨?(\d+(?:,\d{3})*(?:\.\d{2})?)',
        r'Final Cost:\s*‚Ç¨?(\d+(?:,\d{3})*(?:\.\d{2})?)',
        # For solver outputs that show optimization results
        r'Optimal solution found.*objective\s+(\d+(?:\.\d+)?)',
        r'Best objective\s+(\d+(?:\.\d+)?)',
    ]
    
    for pattern in cost_patterns:
        matches = re.findall(pattern, plan_text, re.IGNORECASE | re.MULTILINE)
        if matches:
            # Take the last (most recent) cost found
            cost_str = matches[-1].replace(',', '')
            try:
                cost = float(cost_str)
                # If this contradicts the breakdown calculation, prefer breakdown
                if breakdown and cost == 0 and sum(breakdown.values()) > 0:
                    return sum(breakdown.values())
                return cost
            except ValueError:
                continue
    
    # If no specific cost pattern found, look for any euro amounts and take the largest
    euro_amounts = re.findall(r'‚Ç¨(\d+(?:,\d{3})*(?:\.\d{2})?)', plan_text)
    if euro_amounts:
        amounts = [float(amount.replace(',', '')) for amount in euro_amounts]
        # Return the largest amount as likely total cost
        return max(amounts)
    
    return 0.0

def extract_destination_from_plan(plan_text: str) -> str:
    """Extract destination from plan text"""
    import re
    
    # Look for patterns like "Destination: Zermatt" or "Resort: Zermatt"
    dest_patterns = [
        r'Destination:\s*([^\n]+)',
        r'Resort:\s*([^\n]+)',
        r'Selected Resort:\s*([^\n]+)'
    ]
    
    for pattern in dest_patterns:
        match = re.search(pattern, plan_text, re.IGNORECASE)
        if match:
            return match.group(1).strip()
    
    return "Unknown"

def extract_days_from_plan(plan_text: str) -> int:
    """Extract number of days from plan text"""
    import re
    
    # Look for patterns like "Duration: 5 days" or "5-day"
    days_patterns = [
        r'Duration:\s*(\d+)\s*days?',
        r'(\d+)-day',
        r'(\d+)\s*days?'
    ]
    
    for pattern in days_patterns:
        match = re.search(pattern, plan_text, re.IGNORECASE)
        if match:
            return int(match.group(1))
    
    return 0

def extract_people_from_plan(plan_text: str) -> int:
    """Extract number of people from plan text"""
    import re
    
    # Look for patterns like "People: 4" or "4 people"
    people_patterns = [
        r'People:\s*(\d+)',
        r'(\d+)\s*people'
    ]
    
    for pattern in people_patterns:
        match = re.search(pattern, plan_text, re.IGNORECASE)
        if match:
            return int(match.group(1))
    
    return 0

def extract_budget_from_plan(plan_text: str) -> float:
    """Extract budget from plan text"""
    import re
    
    # Look for patterns like "Budget: ‚Ç¨5000" or "Budget: 5000 euros"
    budget_patterns = [
        r'Budget:\s*‚Ç¨?(\d+\.?\d*)',
        r'(\d+\.?\d*)\s*euros?'
    ]
    
    for pattern in budget_patterns:
        match = re.search(pattern, plan_text, re.IGNORECASE)
        if match:
            return float(match.group(1))
    
    return 0.0

def run_benchmark_comparison(query: str, model_name: str = "gpt-4o-mini", verbose: bool = False) -> ThreeWayBenchmarkComparison:
    """Run all three planners on the same query and compare results"""
    
    print(f"\n{'='*80}")
    print(f"THREE-WAY BENCHMARKING: {query}")
    print(f"Model: {model_name}")
    print(f"{'='*80}")
    
    # Run Pure LLM planner
    print("\nü§ñ Running Pure LLM Planner...")
    pure_llm_result = run_single_planner(pipeline_ski_pure_llm, "Pure LLM", query, model_name, verbose)
    
    if pure_llm_result.success:
        print(f"‚úÖ Pure LLM completed in {pure_llm_result.execution_time:.2f}s - Cost: ‚Ç¨{pure_llm_result.total_cost:.2f}")
    else:
        print(f"‚ùå Pure LLM failed in {pure_llm_result.execution_time:.2f}s - Error: {pure_llm_result.error_message}")
    
    # Run Z3 planner
    print("\nüîç Running Z3 Planner...")
    z3_result = run_single_planner(pipeline_ski_z3, "Z3", query, model_name, verbose)
    
    if z3_result.success:
        print(f"‚úÖ Z3 completed in {z3_result.execution_time:.2f}s - Cost: ‚Ç¨{z3_result.total_cost:.2f}")
    else:
        print(f"‚ùå Z3 failed in {z3_result.execution_time:.2f}s - Error: {z3_result.error_message}")
    
    # Run Gurobi planner
    print("\nüîç Running Gurobi Planner...")
    gurobi_result = run_single_planner(pipeline_ski_gurobi, "Gurobi", query, model_name, verbose)
    
    if gurobi_result.success:
        print(f"‚úÖ Gurobi completed in {gurobi_result.execution_time:.2f}s - Cost: ‚Ç¨{gurobi_result.total_cost:.2f}")
    else:
        print(f"‚ùå Gurobi failed in {gurobi_result.execution_time:.2f}s - Error: {gurobi_result.error_message}")
    
    # Create three-way comparison
    comparison = ThreeWayBenchmarkComparison(query, pure_llm_result, z3_result, gurobi_result)
    
    return comparison

def print_comparison_summary(comparison: ThreeWayBenchmarkComparison):
    """Print a formatted summary of the three-way comparison"""
    analysis = comparison.analyze_comparison()
    
    print(f"\n{'='*80}")
    print("THREE-WAY BENCHMARK RESULTS SUMMARY")
    print(f"{'='*80}")
    
    print(f"Query: {comparison.query}")
    print(f"Timestamp: {comparison.timestamp}")
    
    print("\nüìä EXECUTION RESULTS:")
    print(f"  Pure LLM: {'‚úÖ Success' if comparison.pure_llm_result.success else '‚ùå Failed'}")
    print(f"  Z3:       {'‚úÖ Success' if comparison.z3_result.success else '‚ùå Failed'}")  
    print(f"  Gurobi:   {'‚úÖ Success' if comparison.gurobi_result.success else '‚ùå Failed'}")
    
    success_count = analysis["success_summary"]["success_count"]
    print(f"  Success Rate: {success_count}/3 ({success_count/3*100:.1f}%)")
    
    if success_count >= 2:
        print("\n‚è±Ô∏è  PERFORMANCE COMPARISON:")
        perf = analysis["performance_comparison"]
        if perf.get("pure_llm_time") is not None:
            print(f"  Pure LLM Time: {perf['pure_llm_time']:.2f}s")
        if perf.get("z3_time") is not None:
            print(f"  Z3 Time:       {perf['z3_time']:.2f}s")
        if perf.get("gurobi_time") is not None:
            print(f"  Gurobi Time:   {perf['gurobi_time']:.2f}s")
        print(f"  Fastest:       {perf.get('fastest', 'N/A')} ({perf.get('fastest_time', 0):.2f}s)")
        
        print("\nüí∞ COST COMPARISON:")
        cost = analysis["cost_comparison"]
        if cost.get("pure_llm_cost") is not None and cost["pure_llm_cost"] > 0:
            print(f"  Pure LLM Cost: ‚Ç¨{cost['pure_llm_cost']:.2f}")
            if comparison.pure_llm_result.cost_breakdown:
                for category, amount in comparison.pure_llm_result.cost_breakdown.items():
                    print(f"    - {category.title()}: ‚Ç¨{amount:.2f}")
        if cost.get("z3_cost") is not None and cost["z3_cost"] > 0:
            print(f"  Z3 Cost:       ‚Ç¨{cost['z3_cost']:.2f}")
            if comparison.z3_result.cost_breakdown:
                for category, amount in comparison.z3_result.cost_breakdown.items():
                    print(f"    - {category.title()}: ‚Ç¨{amount:.2f}")
        if cost.get("gurobi_cost") is not None and cost["gurobi_cost"] > 0:
            print(f"  Gurobi Cost:   ‚Ç¨{cost['gurobi_cost']:.2f}")
            if comparison.gurobi_result.cost_breakdown:
                for category, amount in comparison.gurobi_result.cost_breakdown.items():
                    print(f"    - {category.title()}: ‚Ç¨{amount:.2f}")
        if cost.get("cheapest"):
            print(f"  Cheapest:      {cost['cheapest']} (‚Ç¨{cost.get('cheapest_cost', 0):.2f})")
    
    print(f"\nüèÜ WINNER: {analysis['winner']}")
    print(f"üí° RECOMMENDATION: {analysis['recommendation']}")
    
    if success_count < 3:
        print("\n‚ùå ERRORS:")
        if not comparison.pure_llm_result.success:
            print(f"  Pure LLM Error: {comparison.pure_llm_result.error_message}")
        if not comparison.z3_result.success:
            print(f"  Z3 Error: {comparison.z3_result.error_message}")
        if not comparison.gurobi_result.success:
            print(f"  Gurobi Error: {comparison.gurobi_result.error_message}")

def save_benchmark_results(comparison: ThreeWayBenchmarkComparison, output_dir: str = "benchmark_results"):
    """Save benchmark results to JSON file"""
    if not os.path.exists(output_dir):
        os.makedirs(output_dir)
    
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    filename = f"benchmark_{timestamp}.json"
    filepath = os.path.join(output_dir, filename)
    
    with open(filepath, 'w', encoding='utf-8') as f:
        json.dump(comparison.to_dict(), f, indent=2, ensure_ascii=False)
    
    print(f"\nüíæ Results saved to: {filepath}")
    return filepath

def run_batch_benchmark(queries: List[str], model_name: str = "gpt-4o-mini", verbose: bool = False) -> List[ThreeWayBenchmarkComparison]:
    """Run benchmark on multiple queries"""
    results = []
    
    print(f"\nüöÄ Starting batch benchmark with {len(queries)} queries...")
    
    for i, query in enumerate(queries, 1):
        print(f"\nüìã Processing query {i}/{len(queries)}")
        comparison = run_benchmark_comparison(query, model_name, verbose)
        results.append(comparison)
        
        # Save individual result
        save_benchmark_results(comparison)
        
        # Print summary
        print_comparison_summary(comparison)
    
    return results

def generate_batch_report(results: List[ThreeWayBenchmarkComparison], output_dir: str = "benchmark_results"):
    """Generate a comprehensive batch report"""
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    
    # Aggregate statistics
    total_queries = len(results)
    pure_llm_successes = sum(1 for r in results if r.pure_llm_result.success)
    z3_successes = sum(1 for r in results if r.z3_result.success)
    gurobi_successes = sum(1 for r in results if r.gurobi_result.success)
    all_successful = sum(1 for r in results if r.pure_llm_result.success and r.z3_result.success and r.gurobi_result.success)
    
    # Calculate averages only for successful runs
    pure_llm_avg_time = sum(r.pure_llm_result.execution_time for r in results if r.pure_llm_result.success) / max(pure_llm_successes, 1)
    z3_avg_time = sum(r.z3_result.execution_time for r in results if r.z3_result.success) / max(z3_successes, 1)
    gurobi_avg_time = sum(r.gurobi_result.execution_time for r in results if r.gurobi_result.success) / max(gurobi_successes, 1)
    
    pure_llm_avg_cost = sum(r.pure_llm_result.total_cost for r in results if r.pure_llm_result.success and r.pure_llm_result.total_cost > 0) / max(sum(1 for r in results if r.pure_llm_result.success and r.pure_llm_result.total_cost > 0), 1)
    z3_avg_cost = sum(r.z3_result.total_cost for r in results if r.z3_result.success and r.z3_result.total_cost > 0) / max(sum(1 for r in results if r.z3_result.success and r.z3_result.total_cost > 0), 1)
    gurobi_avg_cost = sum(r.gurobi_result.total_cost for r in results if r.gurobi_result.success and r.gurobi_result.total_cost > 0) / max(sum(1 for r in results if r.gurobi_result.success and r.gurobi_result.total_cost > 0), 1)
    
    winners = [r.analyze_comparison()["winner"] for r in results]
    pure_llm_wins = winners.count("Pure LLM")
    z3_wins = winners.count("Z3")
    gurobi_wins = winners.count("Gurobi")
    ties = winners.count("Mixed results") + winners.count("Tie")
    none_wins = winners.count("None")
    
    # Create report
    report = {
        "timestamp": timestamp,
        "total_queries": total_queries,
        "success_rates": {
            "pure_llm_success_rate": pure_llm_successes / total_queries * 100,
            "z3_success_rate": z3_successes / total_queries * 100,
            "gurobi_success_rate": gurobi_successes / total_queries * 100,
            "all_successful_rate": all_successful / total_queries * 100
        },
        "performance_averages": {
            "pure_llm_avg_time": pure_llm_avg_time,
            "z3_avg_time": z3_avg_time,
            "gurobi_avg_time": gurobi_avg_time,
            "pure_llm_avg_cost": pure_llm_avg_cost,
            "z3_avg_cost": z3_avg_cost,
            "gurobi_avg_cost": gurobi_avg_cost
        },
        "winner_distribution": {
            "pure_llm_wins": pure_llm_wins,
            "z3_wins": z3_wins,
            "gurobi_wins": gurobi_wins,
            "ties": ties,
            "none_wins": none_wins,
            "pure_llm_win_rate": pure_llm_wins / total_queries * 100,
            "z3_win_rate": z3_wins / total_queries * 100,
            "gurobi_win_rate": gurobi_wins / total_queries * 100
        },
        "detailed_results": [r.to_dict() for r in results]
    }
    
    # Save report
    report_filename = f"batch_report_{timestamp}.json"
    report_filepath = os.path.join(output_dir, report_filename)
    
    with open(report_filepath, 'w', encoding='utf-8') as f:
        json.dump(report, f, indent=2, ensure_ascii=False)
    
    # Print summary
    print(f"\n{'='*80}")
    print("THREE-WAY BATCH BENCHMARK REPORT")
    print(f"{'='*80}")
    print(f"Total Queries: {total_queries}")
    print(f"Pure LLM Success Rate: {pure_llm_successes}/{total_queries} ({pure_llm_successes/total_queries*100:.1f}%)")
    print(f"Z3 Success Rate: {z3_successes}/{total_queries} ({z3_successes/total_queries*100:.1f}%)")
    print(f"Gurobi Success Rate: {gurobi_successes}/{total_queries} ({gurobi_successes/total_queries*100:.1f}%)")
    print(f"All Successful: {all_successful}/{total_queries} ({all_successful/total_queries*100:.1f}%)")
    
    print("\n‚è±Ô∏è  Average Execution Time:")
    print(f"  Pure LLM: {pure_llm_avg_time:.2f}s")
    print(f"  Z3: {z3_avg_time:.2f}s")
    print(f"  Gurobi: {gurobi_avg_time:.2f}s")
    
    print("\nüí∞ Average Cost:")
    print(f"  Pure LLM: ‚Ç¨{pure_llm_avg_cost:.2f}")
    print(f"  Z3: ‚Ç¨{z3_avg_cost:.2f}")
    print(f"  Gurobi: ‚Ç¨{gurobi_avg_cost:.2f}")
    
    print("\nüèÜ Winner Distribution:")
    print(f"  Pure LLM Wins: {pure_llm_wins} ({pure_llm_wins/total_queries*100:.1f}%)")
    print(f"  Z3 Wins: {z3_wins} ({z3_wins/total_queries*100:.1f}%)")
    print(f"  Gurobi Wins: {gurobi_wins} ({gurobi_wins/total_queries*100:.1f}%)")
    print(f"  Ties/Mixed: {ties} ({ties/total_queries*100:.1f}%)")
    print(f"  All Failed: {none_wins} ({none_wins/total_queries*100:.1f}%)")
    
    print(f"\nüíæ Detailed report saved to: {report_filepath}")
    
    return report

def main():
    parser = argparse.ArgumentParser(description="Pure LLM vs Z3 vs Gurobi Ski Planner Benchmark")
    parser.add_argument("--query", type=str, help="Single query to benchmark")
    parser.add_argument("--query_file", type=str, help="File containing a single query")
    parser.add_argument("--batch_test", action="store_true", help="Run batch test with default queries")
    parser.add_argument("--batch_file", type=str, help="File containing multiple queries (one per line)")
    parser.add_argument("--model_name", type=str, default="gpt-4o-mini", help="LLM model to use")
    parser.add_argument("--max_queries", type=int, default=5, help="Maximum queries for batch test")
    parser.add_argument("--verbose", action="store_true", help="Enable verbose output")
    parser.add_argument("--output_dir", type=str, default="benchmark_results", help="Output directory for results")
    
    args = parser.parse_args()
    
    print("üéø SKI PLANNER THREE-WAY BENCHMARK: Pure LLM vs Z3 vs Gurobi")
    print(f"Model: {args.model_name}")
    print(f"Output Directory: {args.output_dir}")
    
    # Ensure output directory exists
    if not os.path.exists(args.output_dir):
        os.makedirs(args.output_dir)
    
    # Single query benchmark
    if args.query:
        comparison = run_benchmark_comparison(args.query, args.model_name, args.verbose)
        print_comparison_summary(comparison)
        save_benchmark_results(comparison, args.output_dir)
    
    # Single query from file
    elif args.query_file:
        try:
            with open(args.query_file, 'r', encoding='utf-8') as f:
                query = f.read().strip()
            comparison = run_benchmark_comparison(query, args.model_name, args.verbose)
            print_comparison_summary(comparison)
            save_benchmark_results(comparison, args.output_dir)
        except Exception as e:
            print(f"Error reading query file: {e}")
            sys.exit(1)
    
    # Batch test with default queries
    elif args.batch_test:
        default_queries = [
            "Plan a 3-day ski trip to Livigno for 2 people with budget 1500 euros",
            "Organize a 5-day ski vacation to Zermatt for 4 people with budget 5000 euros and SUV rental",
            "Plan a 7-day ski adventure to Cortina d'Ampezzo for 6 people with budget 8000 euros and equipment rental",
            "Book a 4-day ski trip to Val d'Is√®re for 3 people with budget 3500 euros and intermediate slopes",
            "Plan a 6-day ski holiday to St. Moritz for 5 people with budget 7000 euros and luxury accommodation"
        ]
        
        queries = default_queries[:args.max_queries]
        results = run_batch_benchmark(queries, args.model_name, args.verbose)
        generate_batch_report(results, args.output_dir)
    
    # Batch test from file
    elif args.batch_file:
        try:
            with open(args.batch_file, 'r', encoding='utf-8') as f:
                queries = [line.strip() for line in f if line.strip()]
            
            if args.max_queries:
                queries = queries[:args.max_queries]
            
            results = run_batch_benchmark(queries, args.model_name, args.verbose)
            generate_batch_report(results, args.output_dir)
        except Exception as e:
            print(f"Error reading batch file: {e}")
            sys.exit(1)
    
    else:
        print("Please specify a query option. Use --help for usage information.")
        sys.exit(1)

if __name__ == "__main__":
    main()
