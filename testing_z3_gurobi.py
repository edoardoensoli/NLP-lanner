"""
Unified Benchmarking Script for Z3 vs Gurobi Ski Planners
========================================================

This script runs both Z3 and Gurobi planners on the same query and compares:
- Solution quality
- Execution time
- Cost optimization
- Feasibility
- API costs (LLM calls)

Usage:
    python testing_z3_gurobi.py --query "Plan a 5-day ski trip to Zermatt for 4 people with budget 5000 euros"
    python testing_z3_gurobi.py --batch_test --max_queries 10
    python testing_z3_gurobi.py --query_file queries.txt
"""

import os
import sys
import time
import json
import argparse
import traceback
from datetime import datetime
from typing import Dict, List, Tuple, Optional
import pandas as pd

# Add project paths
sys.path.append(os.path.abspath(os.path.join(os.getcwd(), "..")))
sys.path.append(os.path.abspath(os.path.join(os.getcwd(), "tools/planner")))
os.chdir(os.path.dirname(os.path.abspath(__file__)))

# Import both planners
from test_skiplanner_z3_working import pipeline_ski as pipeline_ski_z3
from test_skiplanner_gurobi import pipeline_ski as pipeline_ski_gurobi

# Import LLM client for pure LLM comparison
from test_skiplanner_gurobi import LLMClient

# Import metrics system
try:
    from ski_planner_metrics import SkiPlannerMetrics
    METRICS_AVAILABLE = True
except ImportError:
    print("Warning: ski_planner_metrics not available. Metrics will be disabled.")
    METRICS_AVAILABLE = False
    
    # Mock SkiPlannerMetrics for fallback
    class SkiPlannerMetrics:
        def evaluate_all_metrics(self, **kwargs):
            return {
                'final_pass_rate': 0.0,
                'delivery_rate': 0.0,
                'hard_constraint_pass_rate_micro': 0.0,
                'hard_constraint_pass_rate_macro': 0.0,
                'commonsense_pass_rate': 0.0,
                'interactive_plan_repair_success': 0.0,
                'optimality': 0.0,
                'runtime': kwargs.get('execution_time', 0.0),
                'cost': kwargs.get('total_cost', 0.0)
            }

def pipeline_ski_pure_llm(query: str, mode: str, model_name: str, index: int, model_version: str = None, verbose: bool = False) -> str:
    """Pure LLM pipeline without any constraint solver"""
    try:
        # Initialize LLM client
        llm_client = LLMClient(model_name=model_version or model_name)
        
        # Create a comprehensive prompt for ski trip planning
        planning_prompt = f"""You are an expert ski trip planner. Based on the following query, create a detailed ski trip plan with REALISTIC cost estimates.

Query: {query}

Please provide a comprehensive ski trip plan that includes:
1. Destination and resort selection
2. Accommodation recommendations with specific costs per night
3. Ski slope recommendations based on skill level
4. Equipment rental suggestions with daily costs
5. Transportation options with specific costs
6. Lift ticket prices per person per day
7. Food and dining cost estimates
8. Detailed cost breakdown and total estimated cost

IMPORTANT: Use REALISTIC European market prices. For example:
- Budget hotels: â‚¬80-120 per night for 2 people
- Mid-range hotels: â‚¬150-250 per night for 2 people
- Equipment rental: â‚¬25-40 per person per day
- Lift tickets: â‚¬45-65 per person per day
- Car rental: â‚¬40-80 per day
- Meals: â‚¬25-50 per person per day

Format your response with clear cost breakdowns:
Accommodation Cost: â‚¬X.XX
Equipment Cost: â‚¬X.XX
Transportation Cost: â‚¬X.XX
Lift Tickets Cost: â‚¬X.XX
Food Cost: â‚¬X.XX
Total Cost: â‚¬X.XX

Make sure all costs add up correctly and consider budget constraints, group size, and specific requirements mentioned in the query.

PLAN:"""

        # Get LLM response
        if verbose:
            print("ğŸ¤– Calling Pure LLM for ski trip planning...")
        
        response = llm_client._query_api(planning_prompt)
        
        if response:
            # Format the response as a proper plan
            plan = f"""PURE LLM SKI TRIP PLAN:
{response}

Generated by: Pure LLM ({model_version or model_name})
No constraint solver used - based purely on LLM reasoning."""
            
            return plan
        else:
            return None
            
    except Exception as e:
        if verbose:
            print(f"Error in Pure LLM pipeline: {e}")
        return None

class SkiBenchmarkResult:
    """Container for benchmark results from a single planner"""
    def __init__(self, planner_name: str):
        self.planner_name = planner_name
        self.success = False
        self.execution_time = 0.0
        self.plan_text = ""
        self.error_message = ""
        self.total_cost = 0.0
        self.cost_breakdown = {}
        self.feasible = False
        self.llm_calls = 0
        self.destination = ""
        self.days = 0
        self.people = 0
        self.budget = 0
        self.model_used = ""
        self.status = ""
        
    def to_dict(self) -> Dict:
        """Convert to dictionary for JSON serialization"""
        return {
            "planner_name": self.planner_name,
            "success": self.success,
            "execution_time": self.execution_time,
            "plan_text": self.plan_text,
            "error_message": self.error_message,
            "total_cost": self.total_cost,
            "cost_breakdown": self.cost_breakdown,
            "feasible": self.feasible,
            "status": self.status,
            "llm_calls": self.llm_calls,
            "destination": self.destination,
            "days": self.days,
            "people": self.people,
            "budget": self.budget,
            "model_used": self.model_used
        }

class ThreeWayBenchmarkComparison:
    """Container for comparing Pure LLM vs Z3 vs Gurobi results"""
    def __init__(self, query: str, pure_llm_result: SkiBenchmarkResult, z3_result: SkiBenchmarkResult, gurobi_result: SkiBenchmarkResult):
        self.query = query
        self.pure_llm_result = pure_llm_result
        self.z3_result = z3_result
        self.gurobi_result = gurobi_result
        self.timestamp = datetime.now().isoformat()
        
        # Initialize metrics evaluator
        self.metrics_evaluator = SkiPlannerMetrics()
        
        # Calculate metrics for each planner
        self.pure_llm_metrics = self._calculate_metrics(pure_llm_result)
        self.z3_metrics = self._calculate_metrics(z3_result)
        self.gurobi_metrics = self._calculate_metrics(gurobi_result)
    
    def _calculate_metrics(self, result: SkiBenchmarkResult) -> Dict:
        """Calculate metrics for a single planner result"""
        return self.metrics_evaluator.evaluate_all_metrics(
            query=self.query,
            plan_text=result.plan_text,
            execution_time=result.execution_time,
            success=result.success,
            feasible=result.feasible,
            status=result.status,
            error_message=result.error_message,
            total_cost=result.total_cost,
            llm_calls=result.llm_calls,
            model_used=result.model_used
        )
        
    def analyze_comparison(self) -> Dict:
        """Analyze and compare the three results based on solver reliability."""
        analysis = {
            "query": self.query,
            "timestamp": self.timestamp,
            "winner": "None",
            "recommendation": "",
            "results_summary": {
                "pure_llm": self.pure_llm_result.status,
                "z3": self.z3_result.status,
                "gurobi": self.gurobi_result.status,
            },
            "performance_comparison": {},
            "cost_comparison": {}
        }

        z3_status = self.z3_result.status
        gurobi_status = self.gurobi_result.status
        llm_status = self.pure_llm_result.status

        optimal_solvers = []
        if z3_status == 'optimal':
            optimal_solvers.append(self.z3_result)
        if gurobi_status == 'optimal':
            optimal_solvers.append(self.gurobi_result)

        infeasible_solvers = []
        if z3_status == 'infeasible':
            infeasible_solvers.append(self.z3_result)
        if gurobi_status == 'infeasible':
            infeasible_solvers.append(self.gurobi_result)

        # Case 1: At least one solver found an optimal solution
        if optimal_solvers:
            if len(optimal_solvers) == 2:
                # Both Z3 and Gurobi found a solution, pick the cheapest
                winner_result = min(optimal_solvers, key=lambda r: r.total_cost)
                loser_result = max(optimal_solvers, key=lambda r: r.total_cost)
                analysis["winner"] = winner_result.planner_name
                cost_diff = abs(loser_result.total_cost - winner_result.total_cost)
                
                analysis["recommendation"] = f"{winner_result.planner_name} is recommended, finding an optimal solution with the lowest cost (â‚¬{winner_result.total_cost:.2f})."
                if cost_diff > 1:
                    analysis["recommendation"] += f" It is â‚¬{cost_diff:.2f} cheaper than {loser_result.planner_name}."
                else:
                    # If costs are the same, recommend the faster one
                    faster_res = min(optimal_solvers, key=lambda r: r.execution_time)
                    if faster_res.planner_name != winner_result.planner_name:
                         analysis["recommendation"] += f" However, {faster_res.planner_name} was faster."
                         analysis["winner"] = faster_res.planner_name # The winner is the fastest if costs are the same

            else:
                # Only one solver found a solution
                winner_result = optimal_solvers[0]
                other_solver_status = gurobi_status if winner_result.planner_name == "Z3" else z3_status
                analysis["winner"] = winner_result.planner_name
                analysis["recommendation"] = f"{winner_result.planner_name} is recommended as it was the only solver to find an optimal solution (the other solver returned '{other_solver_status}')."

            # Add performance and cost details for all planners
            analysis["performance_comparison"] = {
                "pure_llm_time": self.pure_llm_result.execution_time,
                "z3_time": self.z3_result.execution_time,
                "gurobi_time": self.gurobi_result.execution_time,
            }
            analysis["cost_comparison"] = {
                "pure_llm_cost": self.pure_llm_result.total_cost if llm_status == 'optimal' else None,
                "z3_cost": self.z3_result.total_cost if z3_status == 'optimal' else None,
                "gurobi_cost": self.gurobi_result.total_cost if gurobi_status == 'optimal' else None,
            }
            return analysis

        # Case 2: No optimal solutions from solvers, but at least one found it infeasible
        if infeasible_solvers:
            analysis["winner"] = "None"
            if len(infeasible_solvers) == 2:
                analysis["recommendation"] = "Query is infeasible. Both Z3 and Gurobi planners confirmed this."
            else:
                # One is infeasible, the other errored
                winner_result = infeasible_solvers[0]
                other_planner = "Gurobi" if winner_result.planner_name == "Z3" else "Z3"
                analysis["recommendation"] = f"Query is likely infeasible. {winner_result.planner_name} confirmed infeasibility, while {other_planner} encountered an error."

            # Add the suggestion from the first infeasible solver
            suggestion = infeasible_solvers[0].error_message.replace('`', '') # The suggestion is stored here
            if suggestion:
                analysis["recommendation"] += f"\nSuggestion: {suggestion}"

            if llm_status == 'optimal':
                analysis["recommendation"] += "\nNote: The Pure LLM planner generated a plan, but it likely violates one or more constraints."
            return analysis

        # Case 3: Both solvers failed/errored
        analysis["winner"] = "None"
        if llm_status == 'optimal':
            analysis["winner"] = "Pure LLM"
            analysis["recommendation"] = "Both Z3 and Gurobi planners failed. The Pure LLM provided a plan, but its feasibility is not guaranteed and it should be reviewed carefully."
        else:
            analysis["recommendation"] = "All planners failed to produce a valid result for this query."

        return analysis
    
    def to_dict(self) -> Dict:
        """Convert to dictionary for JSON serialization"""
        return {
            "query": self.query,
            "timestamp": self.timestamp,
            "pure_llm_result": self.pure_llm_result.to_dict(),
            "z3_result": self.z3_result.to_dict(),
            "gurobi_result": self.gurobi_result.to_dict(),
            "analysis": self.analyze_comparison(),
            "metrics": {
                "pure_llm_metrics": self.pure_llm_metrics,
                "z3_metrics": self.z3_metrics,
                "gurobi_metrics": self.gurobi_metrics
            }
        }

def run_single_planner(planner_func, planner_name: str, query: str, model_name: str, verbose: bool = False, fallback_models: List[str] = None) -> SkiBenchmarkResult:
    """Run a single planner and capture results"""
    result = SkiBenchmarkResult(planner_name)
    
    try:
        start_time = time.time()
        
        # Handle different function signatures
        if planner_name == "Z3":
            # Z3 planner uses 'model' parameter
            plan_text = planner_func(
                query=query,
                mode="benchmark",
                model=model_name,
                index=1,
                model_version=model_name,
                verbose=verbose,
                fallback_models=fallback_models
            )
        elif planner_name == "Pure LLM":
            # Pure LLM planner uses 'model_name' parameter
            plan_text = planner_func(
                query=query,
                mode="benchmark",
                model_name=model_name,
                index=1,
                model_version=model_name,
                verbose=verbose
            )
        else:
            # Gurobi planner uses 'model' parameter, same as Z3
            plan_text = planner_func(
                query=query,
                mode="benchmark",
                model=model_name, # Corrected from model_name
                index=1,
                model_version=model_name,
                verbose=verbose,
                fallback_models=fallback_models
            )
        
        end_time = time.time()
        result.execution_time = end_time - start_time
        
        if plan_text:
            # Check for infeasibility or failure messages
            if "infeasible" in plan_text.lower():
                result.success = True  # Planner worked correctly
                result.feasible = False
                result.status = "infeasible"
                result.error_message = plan_text  # The suggestion is the message
            elif any(keyword in plan_text.lower() for keyword in ["failed", "error"]):
                result.success = False
                result.feasible = False
                result.status = "error"
                result.error_message = plan_text
            else:
                result.success = True
                result.plan_text = plan_text
                result.feasible = True
                result.status = "optimal"
                
                # Extract cost information from plan text
                result.total_cost = extract_cost_from_plan(plan_text)
                result.cost_breakdown = extract_cost_breakdown_from_plan(plan_text)
                
                # Extract other parameters
                result.destination = extract_destination_from_plan(plan_text)
                result.days = extract_days_from_plan(plan_text)
                result.people = extract_people_from_plan(plan_text)
                result.budget = extract_budget_from_plan(plan_text)
                
        else:
            result.success = False
            result.error_message = "No plan generated"
            result.feasible = False
            result.status = "error"
            
    except Exception as e:
        result.success = False
        result.error_message = str(e)
        result.execution_time = time.time() - start_time if 'start_time' in locals() else 0
        result.feasible = False
        
        if verbose:
            print(f"Error in {planner_name}: {e}")
            traceback.print_exc()
    
    return result

def extract_cost_breakdown_from_plan(plan_text: str) -> Dict[str, float]:
    """Extract detailed cost breakdown from plan text"""
    import re
    
    breakdown = {}
    
    # Look for various cost categories
    cost_categories = {
        'accommodation': [r'Accommodation[^:]*:\s*â‚¬?(\d+(?:,\d{3})*(?:\.\d{2})?)', 
                         r'Hotel[^:]*:\s*â‚¬?(\d+(?:,\d{3})*(?:\.\d{2})?)',
                         r'Lodging[^:]*:\s*â‚¬?(\d+(?:,\d{3})*(?:\.\d{2})?)'],
        'equipment': [r'Equipment[^:]*:\s*â‚¬?(\d+(?:,\d{3})*(?:\.\d{2})?)', 
                     r'Ski Rental[^:]*:\s*â‚¬?(\d+(?:,\d{3})*(?:\.\d{2})?)',
                     r'Gear[^:]*:\s*â‚¬?(\d+(?:,\d{3})*(?:\.\d{2})?)'],
        'transportation': [r'Transportation[^:]*:\s*â‚¬?(\d+(?:,\d{3})*(?:\.\d{2})?)', 
                          r'Car[^:]*:\s*â‚¬?(\d+(?:,\d{3})*(?:\.\d{2})?)',
                          r'Travel[^:]*:\s*â‚¬?(\d+(?:,\d{3})*(?:\.\d{2})?)',
                          r'Car Rental[^:]*:\s*â‚¬?(\d+(?:,\d{3})*(?:\.\d{2})?)'],
        'lift_tickets': [r'Lift[^:]*:\s*â‚¬?(\d+(?:,\d{3})*(?:\.\d{2})?)', 
                        r'Ski Pass[^:]*:\s*â‚¬?(\d+(?:,\d{3})*(?:\.\d{2})?)',
                        r'Tickets[^:]*:\s*â‚¬?(\d+(?:,\d{3})*(?:\.\d{2})?)'],
        'food': [r'Food[^:]*:\s*â‚¬?(\d+(?:,\d{3})*(?:\.\d{2})?)', 
                r'Meals[^:]*:\s*â‚¬?(\d+(?:,\d{3})*(?:\.\d{2})?)',
                r'Dining[^:]*:\s*â‚¬?(\d+(?:,\d{3})*(?:\.\d{2})?)'],
    }
    
    for category, patterns in cost_categories.items():
        for pattern in patterns:
            matches = re.findall(pattern, plan_text, re.IGNORECASE | re.MULTILINE)
            if matches:
                try:
                    cost = float(matches[-1].replace(',', ''))
                    breakdown[category] = cost
                    break
                except (ValueError, IndexError):
                    continue
    
    return breakdown

def extract_cost_from_plan(plan_text: str) -> float:
    """Extract total cost from plan text with better parsing"""
    import re
    
    # First, try to calculate total from cost breakdown (more reliable for Gurobi)
    breakdown = extract_cost_breakdown_from_plan(plan_text)
    if breakdown:
        total_from_breakdown = sum(breakdown.values())
        if total_from_breakdown > 0:
            return total_from_breakdown
    
    # Look for patterns like "Total Cost: â‚¬1234.56" or "Cost: â‚¬1234"
    cost_patterns = [
        r'Total Cost:\s*â‚¬?(\d+(?:,\d{3})*(?:\.\d{2})?)',
        r'Total Budget:\s*â‚¬?(\d+(?:,\d{3})*(?:\.\d{2})?)',
        r'Cost:\s*â‚¬?(\d+(?:,\d{3})*(?:\.\d{2})?)',
        r'Total:\s*â‚¬?(\d+(?:,\d{3})*(?:\.\d{2})?)',
        r'Budget Used:\s*â‚¬?(\d+(?:,\d{3})*(?:\.\d{2})?)',
        r'Final Cost:\s*â‚¬?(\d+(?:,\d{3})*(?:\.\d{2})?)',
        # For solver outputs that show optimization results
        r'Optimal solution found.*objective\s+(\d+(?:\.\d+)?)',
        r'Best objective\s+(\d+(?:\.\d+)?)',
    ]
    
    for pattern in cost_patterns:
        matches = re.findall(pattern, plan_text, re.IGNORECASE | re.MULTILINE)
        if matches:
            # Take the last (most recent) cost found
            cost_str = matches[-1].replace(',', '')
            try:
                cost = float(cost_str)
                # If this contradicts the breakdown calculation, prefer breakdown
                if breakdown and cost == 0 and sum(breakdown.values()) > 0:
                    return sum(breakdown.values())
                return cost
            except ValueError:
                continue
    
    # If no specific cost pattern found, look for any euro amounts and take the largest
    euro_amounts = re.findall(r'â‚¬(\d+(?:,\d{3})*(?:\.\d{2})?)', plan_text)
    if euro_amounts:
        amounts = [float(amount.replace(',', '')) for amount in euro_amounts]
        # Return the largest amount as likely total cost
        return max(amounts)
    
    return 0.0

def extract_destination_from_plan(plan_text: str) -> str:
    """Extract destination from plan text"""
    import re
    
    # Look for patterns like "Destination: Zermatt" or "Resort: Zermatt"
    dest_patterns = [
        r'Destination:\s*([^\n]+)',
        r'Resort:\s*([^\n]+)',
        r'Selected Resort:\s*([^\n]+)'
    ]
    
    for pattern in dest_patterns:
        match = re.search(pattern, plan_text, re.IGNORECASE)
        if match:
            return match.group(1).strip()
    
    return "Unknown"

def extract_days_from_plan(plan_text: str) -> int:
    """Extract number of days from plan text"""
    import re
    
    # Look for patterns like "Duration: 5 days" or "5-day"
    days_patterns = [
        r'Duration:\s*(\d+)\s*days?',
        r'(\d+)-day',
        r'(\d+)\s*days?'
    ]
    
    for pattern in days_patterns:
        match = re.search(pattern, plan_text, re.IGNORECASE)
        if match:
            return int(match.group(1))
    
    return 0

def extract_people_from_plan(plan_text: str) -> int:
    """Extract number of people from plan text"""
    import re
    
    # Look for patterns like "People: 4" or "4 people"
    people_patterns = [
        r'People:\s*(\d+)',
        r'(\d+)\s*people'
    ]
    
    for pattern in people_patterns:
        match = re.search(pattern, plan_text, re.IGNORECASE)
        if match:
            return int(match.group(1))
    
    return 0

def extract_budget_from_plan(plan_text: str) -> float:
    """Extract budget from plan text"""
    import re
    
    # Look for patterns like "Budget: â‚¬5000" or "Budget: 5000 euros"
    budget_patterns = [
        r'Budget:\s*â‚¬?(\d+\.?\d*)',
        r'(\d+\.?\d*)\s*euros?'
    ]
    
    for pattern in budget_patterns:
        match = re.search(pattern, plan_text, re.IGNORECASE)
        if match:
            return float(match.group(1))
    
    return 0.0

def run_benchmark_comparison(query: str, available_models: List[str], verbose: bool = False) -> ThreeWayBenchmarkComparison:
    """Run all three planners on the same query and compare results"""
    
    print(f"\n{'='*80}")
    print(f"THREE-WAY BENCHMARKING: {query}")
    print(f"Available models: {available_models}")
    print(f"{'='*80}")
    
    # Run Pure LLM planner
    print("\nğŸ¤– Running Pure LLM Planner...")
    pure_llm_result = run_single_planner_with_fallback(pipeline_ski_pure_llm, "Pure LLM", query, available_models, verbose)
    
    if pure_llm_result.success:
        print(f"âœ… Pure LLM completed in {pure_llm_result.execution_time:.2f}s with {pure_llm_result.model_used} - Cost: â‚¬{pure_llm_result.total_cost:.2f}")
        if verbose:
            print(f"\nğŸ“‹ Pure LLM Plan:\n{pure_llm_result.plan_text[:500]}..." if len(pure_llm_result.plan_text) > 500 else f"\nğŸ“‹ Pure LLM Plan:\n{pure_llm_result.plan_text}")
    else:
        print(f"âŒ Pure LLM failed in {pure_llm_result.execution_time:.2f}s with {pure_llm_result.model_used} - Error: {pure_llm_result.error_message}")
    
    # Run Z3 planner
    print("\nğŸ” Running Z3 Planner...")
    z3_result = run_single_planner_with_fallback(pipeline_ski_z3, "Z3", query, available_models, verbose)
    
    if z3_result.success:
        print(f"âœ… Z3 completed in {z3_result.execution_time:.2f}s with {z3_result.model_used} - Cost: â‚¬{z3_result.total_cost:.2f}")
        # Extract and show resort name from plan
        if z3_result.plan_text and "SELECTED RESORT:" in z3_result.plan_text:
            resort_line = [line for line in z3_result.plan_text.split('\n') if 'SELECTED RESORT:' in line]
            if resort_line:
                print(f"    Resort: {resort_line[0].replace('SELECTED RESORT:', '').strip()}")
        if verbose:
            print(f"\nğŸ“‹ Z3 Plan:\n{z3_result.plan_text}")
    else:
        print(f"âŒ Z3 failed in {z3_result.execution_time:.2f}s with {z3_result.model_used} - Error: {z3_result.error_message}")
    
    # Run Gurobi planner
    print("\nğŸ” Running Gurobi Planner...")
    gurobi_result = run_single_planner_with_fallback(pipeline_ski_gurobi, "Gurobi", query, available_models, verbose)
    
    if gurobi_result.success:
        print(f"âœ… Gurobi completed in {gurobi_result.execution_time:.2f}s with {gurobi_result.model_used} - Cost: â‚¬{gurobi_result.total_cost:.2f}")
        # Extract and show resort name from plan
        if gurobi_result.plan_text and "**Selected Resort:**" in gurobi_result.plan_text:
            resort_line = [line for line in gurobi_result.plan_text.split('\n') if '**Selected Resort:**' in line]
            if resort_line:
                print(f"    Resort: {resort_line[0].replace('**Selected Resort:**', '').strip()}")
        if verbose:
            print(f"\nğŸ“‹ Gurobi Plan:\n{gurobi_result.plan_text}")
    else:
        print(f"âŒ Gurobi failed in {gurobi_result.execution_time:.2f}s with {gurobi_result.model_used} - Error: {gurobi_result.error_message}")
    
    # Create three-way comparison
    comparison = ThreeWayBenchmarkComparison(query, pure_llm_result, z3_result, gurobi_result)
    
    return comparison

def print_comparison_summary(comparison: ThreeWayBenchmarkComparison):
    """Print a formatted summary of the three-way comparison."""
    analysis = comparison.analyze_comparison()

    print(f"\n{'='*80}")
    print("THREE-WAY BENCHMARK RESULTS SUMMARY")
    print(f"{'='*80}")

    print(f"Query: {comparison.query}")
    print(f"Timestamp: {comparison.timestamp}")

    print("\nğŸ“Š EXECUTION RESULTS:")
    results_summary = analysis["results_summary"]
    
    # Custom printing for each result status
    for planner_name, status in results_summary.items():
        if status == "optimal":
            status_icon = "âœ…"
            status_text = "Success (Optimal)"
        elif status == "infeasible":
            status_icon = "â„¹ï¸"
            status_text = "Infeasible"
        else:
            status_icon = "âŒ"
            status_text = f"Failed ({status})"
        
        # Find the corresponding result object to get the model used
        planner_key = f"{planner_name.lower()}_result"
        if planner_name == "pure_llm":
            planner_key = "pure_llm_result"
        model_used = getattr(comparison, planner_key).model_used or "N/A"

        print(f"  {planner_name.replace('_', ' ').title():<9} {status_icon} {status_text} (Model: {model_used})")

    # Cost comparison for optimal solutions
    cost_comparison = analysis.get("cost_comparison", {})
    if any(cost is not None for cost in cost_comparison.values()):
        print("\nğŸ’° COST COMPARISON (for optimal solutions):")
        if cost_comparison.get("z3_cost") is not None:
            print(f"  Z3 Cost:       â‚¬{cost_comparison['z3_cost']:.2f}")
        if cost_comparison.get("gurobi_cost") is not None:
            print(f"  Gurobi Cost:   â‚¬{cost_comparison['gurobi_cost']:.2f}")
        if cost_comparison.get("pure_llm_cost") is not None:
            print(f"  Pure LLM Cost: â‚¬{cost_comparison['pure_llm_cost']:.2f} (Note: Feasibility not guaranteed)")

    print("\nğŸ† RECOMMENDATION:")
    winner = analysis.get("winner", "None")
    recommendation = analysis.get("recommendation", "No recommendation available.")
    print(f"  Winner: {winner}")
    print(f"  Reasoning: {recommendation}")

    print(f"\n{'='*80}")
    
    # Print detailed metrics summary
    print_metrics_summary(comparison)

def print_metrics_summary(comparison: ThreeWayBenchmarkComparison):
    """Print comprehensive metrics summary for all planners"""
    print(f"\nğŸ“Š COMPREHENSIVE METRICS EVALUATION")
    print(f"{'='*80}")
    
    # Print metrics for each planner
    planners = [
        ("Pure LLM", comparison.pure_llm_metrics),
        ("Z3", comparison.z3_metrics),
        ("Gurobi", comparison.gurobi_metrics)
    ]
    
    for planner_name, metrics in planners:
        print(f"\nğŸ” {planner_name.upper()} METRICS:")
        print(f"  Final Pass Rate: {metrics['final_pass_rate']:.1%}")
        print(f"  Delivery Rate: {metrics['delivery_rate']:.1%}")
        print(f"  Hard Constraint Pass Rate (Micro): {metrics['hard_constraint_pass_rate_micro']:.1%}")
        print(f"  Hard Constraint Pass Rate (Macro): {metrics['hard_constraint_pass_rate_macro']:.1%}")
        print(f"  Commonsense Pass Rate: {metrics['commonsense_pass_rate']:.1%}")
        print(f"  Interactive Plan Repair Success: {metrics['interactive_plan_repair_success']:.1%}")
        print(f"  Optimality Score: {metrics['optimality']:.3f}")
        print(f"  Runtime: {metrics['runtime']:.2f}s")
        print(f"  Cost: â‚¬{metrics['cost']:.2f}")
        
        # Show constraint details if available
        if 'constraint_details' in metrics:
            details = metrics['constraint_details']
            print(f"  Constraint Details:")
            print(f"    Budget: {details['budget_constraint']}")
            print(f"    Dates: {details['dates_constraint']}")
            print(f"    People: {details['people_constraint']}")
            print(f"    Resort: {details['resort_constraint']}")
            print(f"    Days: {details['days_constraint']}")
    
    # Compare key metrics
    print(f"\nâš–ï¸  METRICS COMPARISON:")
    print(f"  Final Pass Rate: Pure LLM={comparison.pure_llm_metrics['final_pass_rate']:.1%}, Z3={comparison.z3_metrics['final_pass_rate']:.1%}, Gurobi={comparison.gurobi_metrics['final_pass_rate']:.1%}")
    print(f"  Delivery Rate: Pure LLM={comparison.pure_llm_metrics['delivery_rate']:.1%}, Z3={comparison.z3_metrics['delivery_rate']:.1%}, Gurobi={comparison.gurobi_metrics['delivery_rate']:.1%}")
    print(f"  Hard Constraint (Micro): Pure LLM={comparison.pure_llm_metrics['hard_constraint_pass_rate_micro']:.1%}, Z3={comparison.z3_metrics['hard_constraint_pass_rate_micro']:.1%}, Gurobi={comparison.gurobi_metrics['hard_constraint_pass_rate_micro']:.1%}")
    print(f"  Optimality: Pure LLM={comparison.pure_llm_metrics['optimality']:.3f}, Z3={comparison.z3_metrics['optimality']:.3f}, Gurobi={comparison.gurobi_metrics['optimality']:.3f}")
    print(f"  Runtime (fastest): Pure LLM={comparison.pure_llm_metrics['runtime']:.2f}s, Z3={comparison.z3_metrics['runtime']:.2f}s, Gurobi={comparison.gurobi_metrics['runtime']:.2f}s")
    
    # Identify best performer for each metric
    print(f"\nğŸ† BEST PERFORMERS:")
    
    # Final Pass Rate
    best_final_pass = max(planners, key=lambda x: x[1]['final_pass_rate'])
    print(f"  Final Pass Rate: {best_final_pass[0]} ({best_final_pass[1]['final_pass_rate']:.1%})")
    
    # Delivery Rate
    best_delivery = max(planners, key=lambda x: x[1]['delivery_rate'])
    print(f"  Delivery Rate: {best_delivery[0]} ({best_delivery[1]['delivery_rate']:.1%})")
    
    # Hard Constraint Pass Rate
    best_hard_constraint = max(planners, key=lambda x: x[1]['hard_constraint_pass_rate_micro'])
    print(f"  Hard Constraint Pass Rate: {best_hard_constraint[0]} ({best_hard_constraint[1]['hard_constraint_pass_rate_micro']:.1%})")
    
    # Optimality
    best_optimality = max(planners, key=lambda x: x[1]['optimality'])
    print(f"  Optimality: {best_optimality[0]} ({best_optimality[1]['optimality']:.3f})")
    
    # Runtime (fastest)
    best_runtime = min(planners, key=lambda x: x[1]['runtime'])
    print(f"  Runtime (Fastest): {best_runtime[0]} ({best_runtime[1]['runtime']:.2f}s)")
    
    print(f"{'='*80}")

def save_benchmark_results(comparison: ThreeWayBenchmarkComparison, output_dir: str = "benchmark_results"):
    """Save benchmark results to JSON file"""
    if not os.path.exists(output_dir):
        os.makedirs(output_dir)
    
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    filename = f"benchmark_{timestamp}.json"
    filepath = os.path.join(output_dir, filename)
    
    with open(filepath, 'w', encoding='utf-8') as f:
        json.dump(comparison.to_dict(), f, indent=2, ensure_ascii=False)
    
    print(f"\nğŸ’¾ Results saved to: {filepath}")
    return filepath

def run_batch_benchmark(queries: List[str], available_models: List[str], verbose: bool = False) -> List[ThreeWayBenchmarkComparison]:
    """Run benchmark on multiple queries"""
    results = []
    
    print(f"\nğŸš€ Starting batch benchmark with {len(queries)} queries...")
    print(f"Available models: {available_models}")
    
    for i, query in enumerate(queries, 1):
        print(f"\nğŸ“‹ Processing query {i}/{len(queries)}")
        comparison = run_benchmark_comparison(query, available_models, verbose)
        results.append(comparison)
        
        # Save individual result
        save_benchmark_results(comparison)
        
        # Print summary
        print_comparison_summary(comparison)

        # Add adaptive delay to avoid hitting API rate limits
        if i < len(queries):
            # Longer delay for more queries to be safer with rate limits
            base_delay = 30  # Increased base delay 
            query_factor = min(i * 3, 60)  # Increase delay more aggressively, max 60s additional
            delay = base_delay + query_factor
            
            print(f"\n...waiting {delay} seconds before next query to avoid rate limits...")
            time.sleep(delay)
            time.sleep(delay)

    return results

def generate_batch_report(results: List[ThreeWayBenchmarkComparison], output_dir: str = "benchmark_results"):
    """Generate a comprehensive batch report"""
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    
    # Aggregate statistics
    total_queries = len(results)
    pure_llm_successes = sum(1 for r in results if r.pure_llm_result.success)
    z3_successes = sum(1 for r in results if r.z3_result.success)
    gurobi_successes = sum(1 for r in results if r.gurobi_result.success)
    all_successful = sum(1 for r in results if r.pure_llm_result.success and r.z3_result.success and r.gurobi_result.success)
    
    # Calculate averages only for successful runs
    pure_llm_avg_time = sum(r.pure_llm_result.execution_time for r in results if r.pure_llm_result.success) / max(pure_llm_successes, 1)
    z3_avg_time = sum(r.z3_result.execution_time for r in results if r.z3_result.success) / max(z3_successes, 1)
    gurobi_avg_time = sum(r.gurobi_result.execution_time for r in results if r.gurobi_result.success) / max(gurobi_successes, 1)
    
    pure_llm_avg_cost = sum(r.pure_llm_result.total_cost for r in results if r.pure_llm_result.success and r.pure_llm_result.total_cost > 0) / max(sum(1 for r in results if r.pure_llm_result.success and r.pure_llm_result.total_cost > 0), 1)
    z3_avg_cost = sum(r.z3_result.total_cost for r in results if r.z3_result.success and r.z3_result.total_cost > 0) / max(sum(1 for r in results if r.z3_result.success and r.z3_result.total_cost > 0), 1)
    gurobi_avg_cost = sum(r.gurobi_result.total_cost for r in results if r.gurobi_result.success and r.gurobi_result.total_cost > 0) / max(sum(1 for r in results if r.gurobi_result.success and r.gurobi_result.total_cost > 0), 1)
    
    # Calculate average metrics across all queries
    def calculate_avg_metrics(results_list, planner_type):
        if not results_list:
            return {}
        
        metrics_key = f"{planner_type}_metrics"
        total_metrics = {}
        
        for result in results_list:
            metrics = getattr(result, metrics_key)
            for key, value in metrics.items():
                if key not in total_metrics:
                    total_metrics[key] = []
                total_metrics[key].append(value)
        
        return {key: sum(values) / len(values) if isinstance(values[0], (int, float)) else values[0] 
                for key, values in total_metrics.items()}
    
    # Calculate average metrics for each planner
    pure_llm_avg_metrics = calculate_avg_metrics(results, "pure_llm")
    z3_avg_metrics = calculate_avg_metrics(results, "z3")
    gurobi_avg_metrics = calculate_avg_metrics(results, "gurobi")
    
    # The winner distribution calculation is no longer needed for the report
    
    # Create report
    report = {
        "timestamp": timestamp,
        "total_queries": total_queries,
        "success_rates": {
            "pure_llm_success_rate": pure_llm_successes / total_queries * 100,
            "z3_success_rate": z3_successes / total_queries * 100,
            "gurobi_success_rate": gurobi_successes / total_queries * 100,
            "all_successful_rate": all_successful / total_queries * 100
        },
        "performance_averages": {
            "pure_llm_avg_time": pure_llm_avg_time,
            "z3_avg_time": z3_avg_time,
            "gurobi_avg_time": gurobi_avg_time,
            "pure_llm_avg_cost": pure_llm_avg_cost,
            "z3_avg_cost": z3_avg_cost,
            "gurobi_avg_cost": gurobi_avg_cost
        },
        "metrics_averages": {
            "pure_llm_metrics": pure_llm_avg_metrics,
            "z3_metrics": z3_avg_metrics,
            "gurobi_metrics": gurobi_avg_metrics
        },
        "detailed_results": [r.to_dict() for r in results]
    }
    
    # Save report
    report_filename = f"batch_report_{timestamp}.json"
    report_filepath = os.path.join(output_dir, report_filename)
    
    with open(report_filepath, 'w', encoding='utf-8') as f:
        json.dump(report, f, indent=2, ensure_ascii=False)
    
    # Print summary
    print(f"\n{'='*80}")
    print("THREE-WAY BATCH BENCHMARK REPORT")
    print(f"{'='*80}")
    print(f"Total Queries: {total_queries}")
    print(f"Pure LLM Success Rate: {pure_llm_successes}/{total_queries} ({pure_llm_successes/total_queries*100:.1f}%)")
    print(f"Z3 Success Rate: {z3_successes}/{total_queries} ({z3_successes/total_queries*100:.1f}%)")
    print(f"Gurobi Success Rate: {gurobi_successes}/{total_queries} ({gurobi_successes/total_queries*100:.1f}%)")
    print(f"All Successful: {all_successful}/{total_queries} ({all_successful/total_queries*100:.1f}%)")
    
    print("\nâ±ï¸  Average Execution Time:")
    print(f"  Pure LLM: {pure_llm_avg_time:.2f}s")
    print(f"  Z3: {z3_avg_time:.2f}s")
    print(f"  Gurobi: {gurobi_avg_time:.2f}s")
    
    print("\nğŸ’° Average Cost:")
    print(f"  Pure LLM: â‚¬{pure_llm_avg_cost:.2f}")
    print(f"  Z3: â‚¬{z3_avg_cost:.2f}")
    print(f"  Gurobi: â‚¬{gurobi_avg_cost:.2f}")
    
    # Print average metrics summary
    print("\nğŸ“Š AVERAGE METRICS ACROSS ALL QUERIES:")
    print("  Final Pass Rate:")
    print(f"    Pure LLM: {pure_llm_avg_metrics.get('final_pass_rate', 0):.1%}")
    print(f"    Z3: {z3_avg_metrics.get('final_pass_rate', 0):.1%}")
    print(f"    Gurobi: {gurobi_avg_metrics.get('final_pass_rate', 0):.1%}")
    
    print("  Delivery Rate:")
    print(f"    Pure LLM: {pure_llm_avg_metrics.get('delivery_rate', 0):.1%}")
    print(f"    Z3: {z3_avg_metrics.get('delivery_rate', 0):.1%}")
    print(f"    Gurobi: {gurobi_avg_metrics.get('delivery_rate', 0):.1%}")
    
    print("  Hard Constraint Pass Rate (Micro):")
    print(f"    Pure LLM: {pure_llm_avg_metrics.get('hard_constraint_pass_rate_micro', 0):.1%}")
    print(f"    Z3: {z3_avg_metrics.get('hard_constraint_pass_rate_micro', 0):.1%}")
    print(f"    Gurobi: {gurobi_avg_metrics.get('hard_constraint_pass_rate_micro', 0):.1%}")
    
    print("  Optimality Score:")
    print(f"    Pure LLM: {pure_llm_avg_metrics.get('optimality', 0):.3f}")
    print(f"    Z3: {z3_avg_metrics.get('optimality', 0):.3f}")
    print(f"    Gurobi: {gurobi_avg_metrics.get('optimality', 0):.3f}")
    
    print(f"\nğŸ’¾ Detailed report saved to: {report_filepath}")
    
    return report

def export_batch_metrics_to_csv(results: List[ThreeWayBenchmarkComparison], output_dir: str = "benchmark_results"):
    """Export batch metrics to CSV for easy analysis"""
    if not results:
        return
    
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    
    # Prepare data for CSV
    csv_data = []
    
    for i, result in enumerate(results):
        # Create row for Pure LLM
        csv_data.append({
            'query_id': i + 1,
            'query': result.query,
            'planner': 'Pure LLM',
            'final_pass_rate': result.pure_llm_metrics['final_pass_rate'],
            'delivery_rate': result.pure_llm_metrics['delivery_rate'],
            'hard_constraint_pass_rate_micro': result.pure_llm_metrics['hard_constraint_pass_rate_micro'],
            'hard_constraint_pass_rate_macro': result.pure_llm_metrics['hard_constraint_pass_rate_macro'],
            'commonsense_pass_rate': result.pure_llm_metrics['commonsense_pass_rate'],
            'interactive_plan_repair_success': result.pure_llm_metrics['interactive_plan_repair_success'],
            'optimality': result.pure_llm_metrics['optimality'],
            'runtime': result.pure_llm_metrics['runtime'],
            'cost': result.pure_llm_metrics['cost'],
            'success': result.pure_llm_result.success,
            'feasible': result.pure_llm_result.feasible,
            'status': result.pure_llm_result.status,
            'model_used': result.pure_llm_result.model_used
        })
        
        # Create row for Z3
        csv_data.append({
            'query_id': i + 1,
            'query': result.query,
            'planner': 'Z3',
            'final_pass_rate': result.z3_metrics['final_pass_rate'],
            'delivery_rate': result.z3_metrics['delivery_rate'],
            'hard_constraint_pass_rate_micro': result.z3_metrics['hard_constraint_pass_rate_micro'],
            'hard_constraint_pass_rate_macro': result.z3_metrics['hard_constraint_pass_rate_macro'],
            'commonsense_pass_rate': result.z3_metrics['commonsense_pass_rate'],
            'interactive_plan_repair_success': result.z3_metrics['interactive_plan_repair_success'],
            'optimality': result.z3_metrics['optimality'],
            'runtime': result.z3_metrics['runtime'],
            'cost': result.z3_metrics['cost'],
            'success': result.z3_result.success,
            'feasible': result.z3_result.feasible,
            'status': result.z3_result.status,
            'model_used': result.z3_result.model_used
        })
        
        # Create row for Gurobi
        csv_data.append({
            'query_id': i + 1,
            'query': result.query,
            'planner': 'Gurobi',
            'final_pass_rate': result.gurobi_metrics['final_pass_rate'],
            'delivery_rate': result.gurobi_metrics['delivery_rate'],
            'hard_constraint_pass_rate_micro': result.gurobi_metrics['hard_constraint_pass_rate_micro'],
            'hard_constraint_pass_rate_macro': result.gurobi_metrics['hard_constraint_pass_rate_macro'],
            'commonsense_pass_rate': result.gurobi_metrics['commonsense_pass_rate'],
            'interactive_plan_repair_success': result.gurobi_metrics['interactive_plan_repair_success'],
            'optimality': result.gurobi_metrics['optimality'],
            'runtime': result.gurobi_metrics['runtime'],
            'cost': result.gurobi_metrics['cost'],
            'success': result.gurobi_result.success,
            'feasible': result.gurobi_result.feasible,
            'status': result.gurobi_result.status,
            'model_used': result.gurobi_result.model_used
        })
    
    # Create DataFrame and save
    df = pd.DataFrame(csv_data)
    csv_filename = f"batch_metrics_{timestamp}.csv"
    csv_filepath = os.path.join(output_dir, csv_filename)
    
    df.to_csv(csv_filepath, index=False)
    print(f"ğŸ“Š Metrics exported to CSV: {csv_filepath}")
    
    return csv_filepath

def is_rate_limit_error(error_message: str) -> bool:
    """Check if error is a rate limit error"""
    if not error_message:
        return False
    
    rate_limit_indicators = [
        "429", "rate limit", "rate_limit", "rateLimitExceeded",
        "too many requests", "quota exceeded", "requests per", 
        "throttled", "rate limiting", "per minute", "per hour", 
        "per day", "usage limit", "api limit", "limit exceeded",
        "try again later", "please retry", "overloaded", 
        "temporarily unavailable", "service unavailable"
    ]
    
    error_lower = error_message.lower()
    return any(indicator in error_lower for indicator in rate_limit_indicators)

def get_next_available_model(available_models: List[str], current_model: str) -> Optional[str]:
    """Get next available model from the list"""
    try:
        current_index = available_models.index(current_model)
        if current_index + 1 < len(available_models):
            return available_models[current_index + 1]
    except ValueError:
        pass
    return None

def run_single_planner_with_fallback(planner_func, planner_name: str, query: str, 
                                   available_models: List[str], verbose: bool = False) -> SkiBenchmarkResult:
    """Run a single planner with model fallback on rate limits"""
    
    attempt_count = 0
    max_attempts_per_model = 2  # Allow retry on transient errors
    
    for model_name in available_models:
        for attempt in range(max_attempts_per_model):
            if attempt > 0:
                delay = min(2 ** attempt, 10)  # Exponential backoff, max 10s
                print(f"  Retrying {model_name} after {delay}s delay...")
                time.sleep(delay)
            else:
                print(f"  Trying model: {model_name}")
            
            result = run_single_planner(planner_func, planner_name, query, model_name, verbose, available_models)
            attempt_count += 1
            
            if result.success:
                result.model_used = model_name
                if attempt_count > 1:
                    print(f"  âœ… Success with {model_name} after {attempt_count} attempts")
                return result
            
            if result.error_message and is_rate_limit_error(result.error_message):
                print(f"  âš ï¸  Rate limit hit for {model_name} (attempt {attempt + 1}/{max_attempts_per_model})")
                if attempt < max_attempts_per_model - 1:
                    continue  # Retry this model
                else:
                    print(f"  âŒ Moving to next model after {max_attempts_per_model} failed attempts")
                    break  # Move to next model
            else:
                # Non-rate-limit error, return the result (don't retry)
                print(f"  âŒ Non-rate-limit error with {model_name}: {result.error_message[:100]}...")
                result.model_used = model_name
                return result
    
    # All models failed
    print(f"  ğŸ’¥ All {len(available_models)} models failed after {attempt_count} total attempts")
    result.model_used = available_models[0] if available_models else "unknown"
    result.error_message = f"All {len(available_models)} models failed. Total attempts: {attempt_count}"
    return result

def main():
    """Main function to run benchmark"""
    parser = argparse.ArgumentParser(description="Run ski trip planner benchmarks.")
    parser.add_argument("--query", type=str, help="Single query to run")
    parser.add_argument("--batch_test", action="store_true", help="Run a batch of predefined test queries")
    parser.add_argument("--query_file", type=str, help="File with queries to run")
    parser.add_argument("--max_queries", type=int, default=10, help="Max queries to run for batch test")
    parser.add_argument("--model", type=str, default="DeepSeek-R1", help="Primary LLM model to use")
    parser.add_argument("--fallback_models", type=str, nargs='+', default=["Phi-3-mini-4k-instruct", "Phi-3-medium-4k-instruct", "Llama-3.2-11B-Vision-Instruct"], help="Fallback models to use when primary model hits rate limits")
    parser.add_argument("--output_dir", type=str, default="benchmark_results", help="Directory to save benchmark results")
    parser.add_argument("--verbose", action="store_true", help="Enable verbose output for debugging")
    
    args = parser.parse_args()
    
    # Set up available models list
    available_models = [args.model] + args.fallback_models
    
    # Select queries based on input method
    if args.query:
        # Single query from command line
        queries = [args.query]
    elif args.query_file:
        # Load queries from file
        try:
            with open(args.query_file, 'r', encoding='utf-8') as f:
                queries = [line.strip() for line in f if line.strip()]
        except Exception as e:
            print(f"Error reading query file: {e}")
            sys.exit(1)
    else:
        # Default batch of test queries
        queries = [
            "Plan a 3-day ski trip to Livigno for 2 people with budget 1500 euros",
            "Organize a 5-day ski vacation to Zermatt for 4 people with budget 5000 euros and SUV rental",
            "Plan a 7-day ski adventure to Cortina d'Ampezzo for 6 people with budget 8000 euros and equipment rental",
            "Book a 4-day ski trip to Val d'IsÃ¨re for 3 people with budget 3500 euros and intermediate slopes",
            "Plan a 6-day ski holiday to St. Moritz for 5 people with budget 7000 euros and luxury accommodation"
        ]
    
    # Limit number of queries if max_queries is set
    if args.max_queries:
        queries = queries[:args.max_queries]
    
    print("ğŸ¿ SKI PLANNER THREE-WAY BENCHMARK: Pure LLM vs Z3 vs Gurobi")
    print(f"Available models: {available_models}")
    print(f"Output Directory: {args.output_dir}")
    
    # Ensure output directory exists
    if not os.path.exists(args.output_dir):
        os.makedirs(args.output_dir)
    
    # Run batch benchmark
    results = run_batch_benchmark(queries, available_models, args.verbose)
    
    # Generate batch report
    generate_batch_report(results, args.output_dir)
    
    # Export metrics to CSV
    export_batch_metrics_to_csv(results, args.output_dir)

if __name__ == "__main__":
    main()
